---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  address: >
    <p>Room 3210 </p>
    <p>428 S Shaw LN</p>
    <p>East Lansing, MI, USA</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am Jinghan Jia, a fourth-year Ph.D. student affiliated with the [OPTML Group](https://www.optml-group.com/) at Michigan State University, guided by Prof. [Sijia Liu](https://lsjxjtu.github.io/). My research focuses on advancing the trustworthiness and operational efficiency of AI systems, with a keen focus on bridging theoretical foundations and real-world applications. **I am actively seeking research collaborations and full‚Äëtime job opportunities where I can apply and expand these research directions.**

I am honored to have been selected as an **Anthropic AI Safety Research Fellow**, an exceptionally competitive program with an acceptance rate likely below 1%, with only around 32 fellows chosen worldwide.

My research interets include following:

üõ°Ô∏è **Trustworthy and Aligned Foundation Models**: My work seeks to improve the reliability, safety, and ethical alignment of foundation models. I focus on machine unlearning, LLM alignment ,privacy-preserving techniques, and the development of robust AI systems that align with human values and can withstand real-world challenges. 

‚ö° **Efficient and Scalable AI Training**: I develop methods for efficient and scalable model training, including memory- and parameter-efficient fine-tuning, model sparsification, and Mixture-of-Experts architectures. This line of research aims to make large-scale models more adaptable and accessible while reducing resource requirements.

üß† **Reasoning and Advanced AI Capabilities**: A key focus of my research is on enhancing the reasoning abilities of LLMs through test-time computation, reasoning-driven training, and reinforcement learning. These approaches aim to empower AI systems to address complex problems with greater transparency, adaptability, and reliability.

üìà **Optimization for Modern AI**: My research explores advanced optimization techniques‚Äîincluding gradient‚Äëfree, zeroth‚Äëorder, and bi‚Äëlevel optimization methods‚Äîto boost performance and scalability across diverse AI applications.

**Research Keywords**: Foundation Models (LLMs / Diffusion Models), Trustworthy AI (Unlearning, Alignment, Privacy), Efficient Training (Sparsification, Memory-/Parameter-Efficient Fine-Tuning, MoE), LLM Reasoning (Test-Time Computing, Reasoning-Enhanced Training), Machine Learning, Zeroth-order Optimization, Bi-level Optimization, Convex/Non-convex Optimization
