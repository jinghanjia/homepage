---
layout: about
title: about
permalink: /
# subtitle: <a href='#'>Affiliations</a>. Address. Contacts. Moto. Etc.

profile:
  align: right
  image: prof_pic.png
  image_circular: false # crops the image to make it circular
  address: >
    <p>Room 3210 </p>
    <p>428 S Shaw LN</p>
    <p>East Lansing, MI, USA</p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am Jinghan Jia, a forth-year Ph.D. student affiliated with the [OPTML Group](https://www.optml-group.com/) at Michigan State University, guided by Prof. [Sijia Liu](https://lsjxjtu.github.io/). My research focuses on advancing the trustworthiness and operational efficiency of AI systems, with a keen focus on bridging theoretical foundations and real-world applications.

My research interets include following:
1. **Trustworthy and Aligned Foundation Models**: My work seeks to improve the reliability, safety, and ethical alignment of foundation models. I focus on machine unlearning, LLM alignment ,privacy-preserving techniques, and the development of robust AI systems that align with human values and can withstand real-world challenges. 
2. **Efficient and Scalable AI Training**: I develop methods for efficient and scalable model training, including memory- and parameter-efficient fine-tuning, model sparsification, and Mixture-of-Experts architectures. This line of research aims to make large-scale models more adaptable and accessible while reducing resource requirements.
3. **Reasoning and Advanced AI Capabilities**: A key focus of my research is on enhancing the reasoning abilities of LLMs through test-time computation, reasoning-driven training, and reinforcement learning. These approaches aim to empower AI systems to address complex problems with greater transparency, adaptability, and reliability.
4. **Optimization for Modern AI**: My research explores advanced optimization techniques—including gradient‑free, zeroth‑order, and bi‑level optimization methods—to boost performance and scalability across diverse AI applications.

**Research Keywords**: Foundation Models (LLMs / Diffusion Models), Trustworthy AI (Unlearning, Alignment, Privacy), Efficient Training (Sparsification, Memory-/Parameter-Efficient Fine-Tuning, MoE), LLM Reasoning (Test-Time Computing, Reasoning-Enhanced Training), Machine Learning, Zeroth-order Optimization, Bi-level Optimization, Convex/Non-convex Optimization