---
---

@inproceedings{zhang2021instabilities,
  abbr={ISMRM},
  title={Instabilities in Conventional Multi-Coil MRI Reconstruction with Small Adversarial Perturbations},
  html={https://arxiv.org/pdf/2102.13066.pdf},
  bibtex_show={true},
  preview={MRI_conventional.png},
  author={Zhang*, Chi and Jia*, Jinghan and Yaman, Burhaneddin and Moeller, Steen and Liu, Sijia and Hong, Mingyi and Ak{\c{c}}akaya, Mehmet},
  booktitle={2021 55th Asilomar Conference on Signals, Systems, and Computers},
  pages={895--899},
  year={2021},
  organization={IEEE}
}
@article{zhang2022robustify,
  abbr={ICLR'22},
  title={How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective},
  author={Zhang, Yimeng and Yao, Yuguang and Jia, Jinghan and Yi, Jinfeng and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
  journal={arXiv preprint arXiv:2203.14195},
  year={2022}
}
@article{liu2024rethinking,
  abbr={arXiv},
  title={Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning},
  author={Fan, Chongyu and Liu, Jiancheng and Lin, Licong and Jia, Jinghan and Zhang, Ruiqi and Mei, Song and Liu, Sijia},
  journal={arXiv preprint arXiv:2410.07163},
  year={2024},
  html={https://arxiv.org/pdf/2410.07163?},
  bibtex_show={true},
  preview={simnpo.png},
  code={https://github.com/OPTML-Group/Unlearn-Simple},
  abstract={In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (e.g., copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that 'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks.}
}


@InProceedings{jia2024wagle,
  title={WAGLE: Strategic Weight Attribution for Effective and Modular Unlearning in Large Language Models},
  author={Jia, Jinghan and Liu, Jiancheng and Zhang, Yihua and Ram, Parikshit and Baracaldo, Nathalie and Liu, Sijia},
  abbr={NeurIPS'24},
  html={https://arxiv.org/pdf/2410.17509},
  year={2024},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  bibtex_show={true},
  code={https://github.com/OPTML-Group/WAGLE},
  preview={wagle.png},
  selected={true},
  abstract={"The need for effective unlearning mechanisms in large language models (LLMs) is increasingly urgent, driven by the necessity to adhere to data regulations and foster ethical generative AI practices. LLM unlearning is designed to reduce the impact of undesirable data influences and associated model capabilities without diminishing the utility of the model if unrelated to the information being forgotten. Despite growing interest, much of the existing research has focused on varied unlearning method designs to boost effectiveness and efficiency. However, the inherent relationship between model weights and LLM unlearning has not been extensively examined. In this paper, we systematically explore how model weights interact with unlearning processes in LLMs and we design the weight attribution-guided LLM unlearning method, WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. By strategically guiding the LLM unlearning across different types of unlearning methods and tasks, WAGLE can erase the undesired content, while maintaining the performance of the original tasks. We refer to the weight attribution-guided LLM unlearning method as WAGLE, which unveils the interconnections between 'influence' of weights and 'influence' of data to forget and retain in LLM generation. Our extensive experiments show that WAGLE boosts unlearning performance across a range of LLM unlearning methods such as gradient difference and (negative) preference optimization, applications such as fictitious unlearning (TOFU benchmark), malicious use prevention (WMDP benchmark), and copyrighted information removal, and models including Zephyr-7b-beta and Llama2-7b. To the best of our knowledge, our work offers the first principled method for attributing and pinpointing the influential weights in enhancing LLM unlearning. It stands in contrast to previous methods that lack weight attribution and simpler weight attribution techniques."}
}

@InProceedings{fan2025towards,
  title={Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond},
  author={Fan, Chongyu* and Jia, Jinghan* and Zhang, Yihua and Ramakrishna, Anil and Hong, Mingyi and Liu, Sijia},
  abbr={ICML'25},
  html={https://arxiv.org/abs/2502.05374},
  year={2025},
  booktitle={The Forty-Second International Conference on Machine Learning},
  bibtex_show={true},
  code={https://github.com/OPTML-Group/Unlearn-Smooth},
  preview={sam.png},
  selected={true},
  abstract={"The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to "relearning" the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning."}
}


@inproceedings{jia2022robustness,
  abbr={TSRML'22},
  title={On the Robustness of deep learning-based MRI Reconstruction to image transformations},
  author={Jia, Jinghan and Hong, Mingyi and Zhang, Yimeng and Ak{\c{c}}akaya, Mehmet and Liu, Sijia},
  poster={../poster/MRI_transformation.png},
  preview={MRI_transformation.png},
  html={https://arxiv.org/pdf/2211.04930.pdf},
  bibtex_show={true},
  journal={arXiv preprint arXiv:2211.04930},
  year={2022}
}
@InProceedings{zhang2024unlearncanvas,
  abbr={NeurIPS'24 D&B},
  title={Unlearncanvas: A stylized image dataset to benchmark machine unlearning for diffusion models},
  author={Zhang, Yihua and Zhang, Yimeng and Yao, Yuguang and Jia, Jinghan and Liu, Jiancheng and Liu, Xiaoming and Liu, Sijia},
  html={https://arxiv.org/pdf/2405.15234},
  year={2024},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks},
  bibtex_show={true},
  code={https://unlearn-canvas.netlify.app/},
  preview={unlearncanvas.png},
}
@InProceedings{zhang2024defensive,
  abbr={NeurIPS'24},
  title={Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models},
  author={Zhang, Yimeng and Chen, Xin and Jia, Jinghan and Zhang, Yihua and Fan, Chongyu and Liu, Jiancheng and Hong, Mingyi and Ding, Ke and Liu, Sijia},
  html={https://arxiv.org/pdf/2405.15234},
  year={2024},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  bibtex_show={true},
  code={https://github.com/OPTML-Group/AdvUnlearn},
  preview={advunlearn.png},
}
@InProceedings{jia2024soul,
  abbr={EMNLP'24},
  title={Soul: Unlocking the power of second-order optimization for llm unlearning},
  author={Jia, Jinghan and Zhang, Yihua and Zhang, Yimeng and Liu, Jiancheng and Runwal, Bharat and Diffenderfer, James and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2404.18239},
  html={https://arxiv.org/pdf/2404.18239},
  year={2024},
  selected={true},
  publisher = {EMNLP 2024},
  booktitle={The 2024 Conference on Empirical Methods in Natural Language Processing},
  code={https://github.com/OPTML-Group/SOUL},
  preview={EMNLP_24_SOUL.png},
  bibtex_show={true}
}

@article{liu2024rethinking,
  abbr={arXiv},
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun* and Jia, Jinghan* and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Xu, Xiaojun and Yao, Yuguang and Li, Hang and Varshney, Kush R and others},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024},
  html={https://arxiv.org/pdf/2402.08787},
  bibtex_show={true},
  preview={llm_unlearning_survey.png},
}

@InProceedings{zhang2023generate,
  abbr={ECCV'24},
  title={To Generate or Not? Safety-Driven Unlearned Diffusion Models Are Still Easy To Generate Unsafe Images... For Now},
  author={Zhang*, Yimeng and Jia*, Jinghan and Chen, Xin and Chen, Aochuan and Zhang, Yihua and Liu, Jiancheng and Ding, Ke and Liu, Sijia},
  html={https://arxiv.org/pdf/2310.11868.pdf},
  bibtex_show={true},
  url = {https://arxiv.org/abs/2310.11868},
  code = {https://github.com/OPTML-Group/Diffusion-MU-Attack},
  selected = {true},
  publisher = {ECCV 2024},
  booktitle={European Conference on Computer Vision},
  preview = {overview_unlearn_diff.png},
  year={2024}
}
@InProceedings{jia2024leveraging,
  abbr={NAACL'24},
  title={Leveraging LLMs for dialogue quality measurement},
  author={Jia, Jinghan and Komma, Abi and Leffel, Timothy and Peng, Xujun and Nagesh, Ajay and Soliman, Tamer and Galstyan, Aram and Kumar, Anoop},
  year={2024},
  selected={true},
  booktitle={2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  preview={dialogue_quality.png},
  html={https://assets.amazon.science/57/b5/2739b7fb46e8ae6c4b2ba08e35f0/leveraging-llms-for-dialogue-quality-measurement.pdf}
}
@InProceedings{chen2023deepzero,
  abbr={ICLR 2024},
  selected={false},
  bibtex_show={true},
  preview={deep_zero.png},
  html={https://arxiv.org/abs/2310.02025},
  code={./},
  title =    {DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training},
  author =       {Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2023},
  abstract =   {Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop DeepZero, a principled ZO deep learning (DL) framework that can scale ZO optimization to DNN training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model pruning methodology using only finite differences to explore and exploit the sparse DL prior in CGE. Third, we develop the methods of feature reuse and forward parallelization to advance the practical implementations of ZO training. Our extensive experiments show that DeepZero achieves state-of-the-art (SOTA) accuracy on ResNet-20 trained on CIFAR-10, approaching FO training performance for the first time. Furthermore, we show the practical utility of DeepZero in applications of certified adversarial defense and DL-based partial differential equation error correction, achieving 10-20% improvement over SOTA. We believe our results will inspire future research on scalable ZO optimization and contribute to advancing DL with black box.}
}
@InProceedings{jia2023model,
  abbr={NeurIPS'23},
  title={Model Sparsity Can Simplify Machine Unlearning},
  author={Jia*, Jinghan and Liu*, Jiancheng and Ram, Parikshit and Yao, Yuguang and Liu, Gaowen and Liu, Yang and Sharma, Pranay and Liu, Sijia},
  html={https://arxiv.org/pdf/2304.04934.pdf},
  year={2023},
  selected={true},
  publisher = {NeurIPS 2023},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  code={https://github.com/OPTML-Group/Unlearn-Sparse},
  preview={unlearn_sparse.jpg},
  bibtex_show={true}
}





@InProceedings{zhang2023selectivity,
  abbr={NeurIPS'23},
  selected={false},
  bibtex_show={true},
  preview={dp_flm.png},
  html={https://arxiv.org/abs/2310.08782},
  code={https://github.com/OPTML-Group/DP4TL},
  title = {Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning},
  author = {Zhang, Yihua and Zhang, Yimeng and Chen, Aochuan and Jia, Jinghan and Liu, Jiancheng, and Liu, Gaowen and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  abstract =   {Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings respectively, by revisiting the DP problem through the lens of source-target domain mapping. Furthermore, we demonstrate the effectiveness of our approach on numerous transfer learning tasks. We show that source data classes can be pruned by up to 40% without sacrificing the downstream performance, resulting in a significant 2~5 times speed-up during the pretraining stage. Besides, our proposal exhibits broad applicability and can improve other computationally intensive transfer learning techniques, such as adversarial pretraining.}
}


@InProceedings{https://doi.org/10.48550/arxiv.2211.11711,
  abbr={SANER'23},
  doi = {10.48550/ARXIV.2211.11711},
  html={https://arxiv.org/pdf/2211.11711.pdf},
  bibtex_show={true},
  url = {https://arxiv.org/abs/2211.11711},
  code={https://github.com/OPTML-Group/CLAW-SAT},
  author = {Jia*, Jinghan and Srikant*, Shashank and Mitrovska, Tamara and Gan, Chuang and Chang, Shiyu and Liu, Sijia and O'Reilly, Una-May},
  poster={../poster/CLAWSAT.png},
  preview={Claw_sat_overview.png},  
  keywords = {Machine Learning (cs.LG), Programming Languages (cs.PL), Software Engineering (cs.SE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  selected={true},
  booktitle={2023 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)},
  title = {CLAWSAT: Towards Both Robust and Accurate Code Models},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}
